{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/1UxvjY5S0P/ytNOlp9O3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasaaz/ACII/blob/main/Pre_Processamento_Textual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Normalização\n",
        "\n",
        "*Realizar as seguintes ações de normalização no arquivo de entrada e gerar o arquivo de saída Shakespeare_Normalized.txt.*\n",
        "\n",
        "\n",
        "* Lower case reduction\n",
        "\n",
        "* Accent and diacritic removal\n",
        "\n",
        "* Canonicalizing of acronyms, currency, date and hyphenated words\n",
        "\n",
        "* Punctuation removal (except currency and date).\n",
        "\n",
        "* Special characters removal"
      ],
      "metadata": {
        "id": "Wq9TSSegRN5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ldrBdCDXGeUp"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    # 1. Lower case reduction\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Accent and diacritic removal\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    # 3. Canonicalizing of acronyms, currency, date and hyphenated words\n",
        "    text = re.sub(r'\\b(\\w+)\\s*-\\s*(\\w+)\\b', r'\\1\\2', text)  # Hyphenated words\n",
        "    text = re.sub(r'\\$\\d+\\.\\d{2}', '[CURRENCY]', text)  # Currency\n",
        "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', '[DATE]', text)  # Date\n",
        "\n",
        "    # 4. Punctuation removal (except currency and date)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 5. Special characters removal\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "with open('Shakespeare.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "\n",
        "with open('Shakespeare_Normalized.txt', 'w') as file:\n",
        "    file.write(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tokenização\n",
        "\n",
        "Realizar cada uma das seguintes tokenizações no arquivo ShakespeareNormalized txt e gerar o arquivo de saída Shakespeare_Normalized_TokenizedXX.txt, onde XX é o número da subtarefa. Por exemplo, o arquivo Shakespeare_Normalized_Tokenized01.txt é a saída do algoritmo 1 (White Space Tokenization):\n",
        "\n",
        "* White Space Tokenization\n",
        "* NLTK: Word Tokenizer\n",
        "* NLTK: Tree Bank Tokenizer\n",
        "* NLTK: Word Punctuation Tokenizer\n",
        "* NLTK: Tweet Tokenizer\n",
        "* NLTK: MWE Tokenizer\n",
        "* TextBlob Word Tokenizer\n",
        "* spaCy Tokenizer\n",
        "* Gensim Word Tokenizer\n",
        "* Keras Tokenization"
      ],
      "metadata": {
        "id": "maEsOTF_R2Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Baixar os pacotes \"punkt\" e \"wordnet\", se não estiverem baixados\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"Caminhos NLTK: \", nltk.data.path)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Baixar o pacote punkt\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Carregar o modelo spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_text(text, method):\n",
        "    if method == 1:\n",
        "        return text.split()\n",
        "    elif method == 2:\n",
        "        return word_tokenize(text)\n",
        "    elif method == 3:\n",
        "        return TreebankWordTokenizer().tokenize(text)\n",
        "    elif method == 4:\n",
        "        return WordPunctTokenizer().tokenize(text)\n",
        "    elif method == 5:\n",
        "        return TweetTokenizer().tokenize(text)\n",
        "    elif method == 6:\n",
        "        return MWETokenizer().tokenize(text.split())\n",
        "    elif method == 7:\n",
        "        return TextBlob(text).words\n",
        "    elif method == 8:\n",
        "        return [token.text for token in nlp(text)]\n",
        "    elif method == 9:\n",
        "        return list(tokenize(text))\n",
        "    elif method == 10:\n",
        "        return text_to_word_sequence(text)\n",
        "\n",
        "methods = {\n",
        "    1: \"Tokenização por Espaço em Branco\",\n",
        "    2: \"Tokenizador de Palavras do NLTK\",\n",
        "    3: \"Tokenizador Tree Bank do NLTK\",\n",
        "    4: \"Tokenizador de Pontuação do NLTK\",\n",
        "    5: \"Tokenizador de Tweet do NLTK\",\n",
        "    6: \"Tokenizador MWE do NLTK\",\n",
        "    7: \"Tokenizador de Palavras do TextBlob\",\n",
        "    8: \"Tokenizador do spaCy\",\n",
        "    9: \"Tokenizador do Gensim\",\n",
        "    10: \"Tokenização do Keras\"\n",
        "}\n",
        "\n",
        "# Verificação do arquivo de texto\n",
        "if os.path.exists('Shakespeare_Normalized.txt'):\n",
        "    with open('Shakespeare_Normalized.txt', 'r') as file:\n",
        "        normalized_text = file.read()\n",
        "\n",
        "    for method_num, method_name in methods.items():\n",
        "        tokens = tokenize_text(normalized_text, method_num)\n",
        "        with open(f'Shakespeare_Normalized_Tokenized{method_num:02d}.txt', 'w') as file:\n",
        "            file.write('\\n'.join(tokens))\n",
        "else:\n",
        "    print(\"O arquivo 'Shakespeare_Normalized.txt' não foi encontrado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAg1xgoGILpd",
        "outputId": "32ba21af-31ed-41a6-e978-b49c8f66a473"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caminhos NLTK:  ['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Stop-words Removal\n",
        "\n",
        "Realizar a remoção de stop-words do texto (apenas o da subtarefa 2 de tokenização), e gerar um arquivo de saída denominado Shakespeare_Normalized_Tokenized_StopWord.txt."
      ],
      "metadata": {
        "id": "FN2cqQbhSH9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized01.txt', 'r') as file:\n",
        "    tokens = file.read().splitlines()\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'w') as file:\n",
        "    file.write('\\n'.join(filtered_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNApBtRpOfyL",
        "outputId": "4a9747fd-06d0-4efe-cc6a-4305eca2404d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Text Lemmatization\n",
        "\n",
        "Realizar a lematização do texto gerado na etapa anterior, utilizando o WordNet Lemmatizer e gerar um arquivo de saída denominado Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt."
      ],
      "metadata": {
        "id": "clt1R-asSSZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'r') as file:\n",
        "    tokens = file.read().splitlines()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w') as file:\n",
        "    file.write('\\n'.join(lemmatized_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHYzaGllOkMK",
        "outputId": "38ba61e7-1ed5-4551-83db-5995180c0c09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Text Stemming\n",
        "\n",
        "Aplicar cada um dos seguintes stemmers no arquivo de entrada Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt e gerar o arquivo de saída Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_StemmingXX.txt, onde XX é o número da subtarefa. Por exemplo, o arquivo Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt é a saída do algoritmo 1 (Porter Stemmer):\n",
        "\n",
        "* Porter Stemmer\n",
        "* Snowball Stemmer"
      ],
      "metadata": {
        "id": "PU9Kyfi9SYqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r') as file:\n",
        "    tokens = file.read().splitlines()\n",
        "\n",
        "porter_stemmed_tokens = [porter.stem(word) for word in tokens]\n",
        "snowball_stemmed_tokens = [snowball.stem(word) for word in tokens]\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w') as file:\n",
        "    file.write('\\n'.join(porter_stemmed_tokens))\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w') as file:\n",
        "    file.write('\\n'.join(snowball_stemmed_tokens))"
      ],
      "metadata": {
        "id": "GnHsmgyfOosr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Análise do Vocabulário\n",
        "\n",
        "Comparar os vocabulários gerados por cada lematizador e stemmer, apresentando um arquivo CSV para cada um deles contendo:\n",
        "\n",
        "* Token (raíz resultante)\n",
        "* Número de ocorrências do token no documento resultante (lematizado ou com stemming)\n",
        "* Tamanho em caracteres de cada token do vocabulário\n",
        "\n",
        "Por exemplo, para o lematizador, gerar o arquivo Shakespeare_Vocabulary_Lemmatized.csv e para o Porter Stemmer gerar o arquivo Shakespeare_Vocabulary_Porter.csv.\n",
        "\n",
        "Apresentar um documento final comparativo denominado Shakespeare_Vocabulary_Analysis.txt contendo, para cada lematizador e stemmer utilizado, o tamanho do vocabulário (número de tokens), o número médio de ocorrências e o tamanho médio dos tokens."
      ],
      "metadata": {
        "id": "6ZPCpzn6SxUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_vocabulary(tokens, output_file):\n",
        "    token_counts = Counter(tokens)\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Token', 'Occurrences', 'Length'])\n",
        "        for token, count in token_counts.items():\n",
        "            writer.writerow([token, count, len(token)])\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r') as file:\n",
        "    lemmatized_tokens = file.read().splitlines()\n",
        "\n",
        "analyze_vocabulary(lemmatized_tokens, 'Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'r') as file:\n",
        "    porter_stemmed_tokens = file.read().splitlines()\n",
        "\n",
        "analyze_vocabulary(porter_stemmed_tokens, 'Shakespeare_Vocabulary_Porter.csv')\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'r') as file:\n",
        "    snowball_stemmed_tokens = file.read().splitlines()\n",
        "\n",
        "analyze_vocabulary(snowball_stemmed_tokens, 'Shakespeare_Vocabulary_Snowball.csv')"
      ],
      "metadata": {
        "id": "3H_GoYEOOtOq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documento Final Comparativo"
      ],
      "metadata": {
        "id": "AZVkSL2ZTFQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_comparative_analysis():\n",
        "    lemmatized_df = pd.read_csv('Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "    porter_df = pd.read_csv('Shakespeare_Vocabulary_Porter.csv')\n",
        "    snowball_df = pd.read_csv('Shakespeare_Vocabulary_Snowball.csv')\n",
        "\n",
        "    with open('Shakespeare_Vocabulary_Analysis.txt', 'w') as file:\n",
        "        file.write(\"Lemmatizer Analysis:\\n\")\n",
        "        file.write(f\"Vocabulary Size: {len(lemmatized_df)}\\n\")\n",
        "        file.write(f\"Average Occurrences: {lemmatized_df['Occurrences'].mean()}\\n\")\n",
        "        file.write(f\"Average Token Length: {lemmatized_df['Length'].mean()}\\n\\n\")\n",
        "\n",
        "        file.write(\"Porter Stemmer Analysis:\\n\")\n",
        "        file.write(f\"Vocabulary Size: {len(porter_df)}\\n\")\n",
        "        file.write(f\"Average Occurrences: {porter_df['Occurrences'].mean()}\\n\")\n",
        "        file.write(f\"Average Token Length: {porter_df['Length'].mean()}\\n\\n\")\n",
        "\n",
        "        file.write(\"Snowball Stemmer Analysis:\\n\")\n",
        "        file.write(f\"Vocabulary Size: {len(snowball_df)}\\n\")\n",
        "        file.write(f\"Average Occurrences: {snowball_df['Occurrences'].mean()}\\n\")\n",
        "        file.write(f\"Average Token Length: {snowball_df['Length'].mean()}\\n\")\n",
        "\n",
        "generate_comparative_analysis()"
      ],
      "metadata": {
        "id": "kvca8lsvOxLh"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}